"""
Contains functions for generating data for DIISCO
"""
import numpy as np
import pandas as pd

import numpy as np

import numpy as np
from jaxtyping import Float

def sample_from_gp(x: Float[np.ndarray, "n_examples, 1"], lengthscale: float, noise: float) -> Float[np.ndarray, "n_examples, 1"]:
    """
    Sample from a Gaussian Process with a standard Gaussian (RBF) kernel.

    Parameters:
    - x : np.ndarray : Input array of shape (n_examples, 1)
    - lengthscale : float : Lengthscale parameter for the RBF kernel
    - noise : float : Noise term for the kernel

    Returns:
    - np.ndarray : Sampled values from the GP of shape (n_examples, 1)
    """

    # Ensure x is a 2D array
    if x.ndim == 1:
        x = x[:, np.newaxis]

    # Number of examples
    n_examples = x.shape[0]

    def rbf_kernel(a: Float[np.ndarray, "n, d"], b: Float[np.ndarray, "m, d"], lengthscale: float) -> Float[np.ndarray, "n, m"]:
        """
        Radial Basis Function (RBF) kernel.

        Parameters:
        - a : np.ndarray : First input array of shape (n, d)
        - b : np.ndarray : Second input array of shape (m, d)
        - lengthscale : float : Lengthscale parameter for the RBF kernel

        Returns:
        - np.ndarray : Covariance matrix of shape (n, m)
        """
        sqdist = np.sum(a**2, 1).reshape(-1, 1) + np.sum(b**2, 1) - 2 * np.dot(a, b.T)
        return np.exp(-0.5 * sqdist / lengthscale**2)

    # Compute the covariance matrix
    K = rbf_kernel(x, x, lengthscale) + noise**2 * np.eye(n_examples)

    # Sample from the multivariate normal distribution
    L = np.linalg.cholesky(K + 1e-6 * np.eye(n_examples))
    sample = L @ np.random.normal(size=(n_examples, 1))

    return sample



def generate_data(n_blocks : int = 30, n_indepenent_per_block : int = 10, n_dependent_per_block : int = 10, n_timepoints : int = 100, noise : float = 0.1, length_scale : float = 1, seed : int = 0, p_active : float = 0.1):
    """
    Generates data for DIISCO

    The data is generated as follows. There are n_blocks. Each block has n_independent_per_block independent
    variables andn_dependent_per_block dependent variables. Withing each block the independent variables
    are sampled from a gaussian process with noise `noise` and length scale `length_scale`. The dependent
    A matrix is then generated by sampling from a gaussian process with noise `noise` and length scale `length_scale`
    using the independent variables as input. The data is generated for n_timepoints timepoints.


    Parameters
    ----------
    n_blocks : int
        A block can be thought of as a cluster of points s
    n_indepenent_per_block : int
        The number of independent variables per block
    n_dependent_per_block : int
        The number of dependent variables per block
    n_timepoints : int
        The number of total timepoints
    noise : float
        The noise in the data
    length_scale : float
        The length scale of the gaussian process

    Returns
    -------
    weights_matrix : np.ndarray
        The weights matrix of shape (n_timepoints, total_cells, total_cells)
    is_active_matrix : np.ndarray
        The is_active_matrix containing 1 if the edge is active, 0 otherwise
        of shape (n_timepoints, total_cells, total
    observed_matrix : np.ndarray
        The observed_matrix containing the observed values of the cells
        of shape (n_timepoints, total_cells)
    timepoints : np.ndarray
        The timepoints of the data of shape (n_timepoints,)
    """
    np.random.seed(seed)

    weights_length_scale = length_scale * 3
    block_size = n_indepenent_per_block + n_dependent_per_block
    total_cells = n_blocks * block_size
    weights_matrix = np.zeros((n_timepoints, total_cells, total_cells))

    is_active_matrix = np.zeros((n_timepoints, total_cells, total_cells)) # 1 if the edge is active, 0 otherwise
    observed_matrix = np.zeros((n_timepoints, total_cells)) # 1 if the cell is observed, 0 otherwise

    timepoints = np.linspace(0, 1, n_timepoints)

    # Fill in the weights matrix
    for i in range(total_cells):
        for j in range(total_cells):
            weights_matrix[:, i, j] = sample_from_gp(timepoints, weights_length_scale, noise).flatten()

    # Lets pretend that they all independent variables at the start
    for cell in range(total_cells):
        observed_matrix[:, cell] = sample_from_gp(timepoints, length_scale, noise).flatten()

    # Now we repace the dependent variables with the independent variables
    for block in range(n_blocks):
        independent_block = np.arange(block * block_size, block * block_size + n_indepenent_per_block)
        dependent_block = np.arange(block * block_size + n_indepenent_per_block, block * block_size + block_size)

        for cell in dependent_block:
            active_independent_cells_mask = np.random.rand(n_indepenent_per_block) < p_active
            for timepoint in range(n_timepoints):
                actual_observed_matrix = observed_matrix[timepoint, independent_block] * active_independent_cells_mask
                observed_matrix[timepoint, cell] = weights_matrix[timepoint, cell, independent_block] @ actual_observed_matrix + np.random.normal(0, noise)
                is_active_matrix[timepoint, cell, independent_block] = active_independent_cells_mask

        is_active_matrix[:, dependent_block, dependent_block] = 1

    assert weights_matrix.shape == (n_timepoints, total_cells, total_cells)
    assert is_active_matrix.shape == (n_timepoints, total_cells, total_cells)
    assert observed_matrix.shape == (n_timepoints, total_cells)
    assert timepoints.shape == (n_timepoints,)

    return weights_matrix, is_active_matrix, observed_matrix, timepoints